{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "# 假设已经有一个embedding层，embedding_dim是嵌入维度\n",
    "embedding_dim = 5\n",
    "embedding = nn.Embedding(10, embedding_dim,padding_idx=0)  # 假设词汇表大小为10\n",
    "\n",
    "# 假设原始的token是3个句子，它们已经被编码为 token ids\n",
    "# 示例token序列为[1, 2, 3], [4, 5], [6, 7, 8, 9]\n",
    "tokens = torch.tensor([[1,2,3,0],\n",
    "                       [4,5,0,0],\n",
    "                       [6,7,8,9]],dtype=torch.int32)\n",
    "\n",
    "# 使用Embedding层将token转换为嵌入\n",
    "embedded_tokens = embedding(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-2.3816,  0.8971,  0.9209, -0.1366, -0.0108],\n",
       "         [-0.7181, -1.2318,  0.2207, -1.4396,  0.4422],\n",
       "         [-2.0836, -0.1481,  1.2221, -0.7765,  0.9815],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        [[-0.3544, -0.3383,  0.1941,  0.3798,  0.6775],\n",
       "         [ 0.2916,  0.6503,  0.8309, -1.5693,  1.0405],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        [[-0.0277,  0.2214, -0.1070,  1.3576, -2.7300],\n",
       "         [ 0.3351,  0.3744,  1.3038, -0.7976,  0.5450],\n",
       "         [ 1.6915, -0.6120,  1.9171, -1.1943, -0.7973],\n",
       "         [-2.2708, -0.7220, -0.8340,  1.7299,  1.3875]]],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-2.3816,  0.8971,  0.9209, -0.1366, -0.0108],\n",
       "         [-0.7181, -1.2318,  0.2207, -1.4396,  0.4422],\n",
       "         [-2.0836, -0.1481,  1.2221, -0.7765,  0.9815],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        [[-0.3544, -0.3383,  0.1941,  0.3798,  0.6775],\n",
       "         [ 0.2916,  0.6503,  0.8309, -1.5693,  1.0405],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        [[-0.0277,  0.2214, -0.1070,  1.3576, -2.7300],\n",
       "         [ 0.3351,  0.3744,  1.3038, -0.7976,  0.5450],\n",
       "         [ 1.6915, -0.6120,  1.9171, -1.1943, -0.7973],\n",
       "         [-2.2708, -0.7220, -0.8340,  1.7299,  1.3875]]],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_inputs  = embedded_tokens \n",
    "padded_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 假设每个序列的有效长度\n",
    "lengths = torch.tensor([3, 2, 4])\n",
    "\n",
    "# 将填充后的输入和有效长度传入pack_padded_sequence\n",
    "packed_input = pack_padded_sequence(padded_inputs, lengths, batch_first=True, enforce_sorted=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([3, 4, 10])\n"
     ]
    }
   ],
   "source": [
    "# 定义一个LSTM层\n",
    "lstm = nn.LSTM(input_size=embedding_dim, hidden_size=10, batch_first=True)\n",
    "\n",
    "# 输入LSTM\n",
    "packed_output, (h_n, c_n) = lstm(packed_input)\n",
    "\n",
    "# 解包输出\n",
    "output, _ = pad_packed_sequence(packed_output, batch_first=True)\n",
    "\n",
    "print(\"Output shape:\", output.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9, 10])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "packed_output[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
