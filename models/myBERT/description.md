# 数据分析

## 总体分析

自己搭建的BRET模型，参数采用官方的超参数（参考文献？）：

- 768个隐藏单元
- 学习率2e-5
- 12个注意力头

[第一个模型](./model1.txt)的学习率为5e-3，一轮训练准确率72%
此后采用官方超参数，batch_size = 64,训练50轮，选择logits的最后一个位置分类后输出,准确率[84%](./model2.txt)
采用[最大池化](./model3.txt)对模型改善不大（？缺少lr=2e-5时选择logits[-1]单轮训练的对照）
增加[梯度裁剪和xavier初始化模型](model5.txt)性能反而下降。最大池化+梯度裁剪训练50轮，准确率[80%](model9.txt)可能是因为transforerEncoder中已经有层规范化(nn.LayerNorm)，所以不需要梯度裁剪。

加入掩蔽后[性能变差](./model11.txt)

## 下一步实验
