# 单轮训练的基本操作

* 首先要有处理好的数据 Dataset DataLoader
* 有一个模型 net
* X->net 得到y_hat, 与y计算差值

  ```python
  #以交叉熵为例
  criterion = torch.nn.CrossEntropyLoss()
  loss = criterion(y, y_hat)
  ```

* 因为网络中参数的梯度是累加的，所以反向传播时要先清零

  ```python
  net.zero_grad()
  loss.backward()
  ```

‍

* 接下来新建一个优化器

  ```python
  optimizer = torch.optim.SGD(net.parameters(),lr=0.01)
  ```

* 完整过程如下

  ```python
  optimizer = torch.optim.SGD(net.parameters(),lr=0.01)
  optimizer.zero_grad()
  y_hat = net(X)
  criterion = torch.nn.CrossEntropyLoss()
  loss = criterion(y, y_hat)
  loss.backward()
  optimizer.step()
  ```
## 常用损失函数
### nn.CrossEntropy()
### nn.MSELoss()均方损失
在 PyTorch 中，`nn.MSELoss` 是一个用于计算均方误差（Mean Squared Error, MSE）的损失函数。它通常用于回归问题，以衡量预测值和目标值之间的差异。MSE 的计算方式是将预测值与目标值之间的差异平方并取平均值。

#### 语法

```python
loss = nn.MSELoss(reduction='none')
```

#### 参数说明

- **`reduction`**：指定损失的计算方式。它可以取以下值：
  - `'mean'`：计算所有元素的均方误差并返回平均值（这是默认值）。
  - `'sum'`：计算所有元素的均方误差并返回总和。
  - `'none'`：不进行任何归约操作，返回每个元素的损失值（例如逐元素的平方误差）。

#### `reduction='none'` 的含义

使用 `reduction='none'`，损失函数会返回逐元素的平方误差，而不会进行任何聚合。这种方式返回的输出张量与输入张量的大小相同，其中每个元素表示预测和目标之间的平方误差。

这种设置适用于需要逐元素误差的场景，例如：
- **逐元素分析误差**：可以查看每个数据点的具体误差大小。
- **自定义损失聚合**：在某些情况下可能希望根据不同的数据点权重或条件对损失进行进一步处理。

#### 示例

假设有预测值 `preds` 和真实值 `targets`，我们可以使用 `nn.MSELoss(reduction='none')` 来计算逐元素的平方误差：

```python
import torch
import torch.nn as nn

# 预测值和真实值
preds = torch.tensor([2.5, 0.0, 2.0, 8.0])
targets = torch.tensor([3.0, -0.5, 2.0, 7.0])

# 定义 MSELoss 并设置 reduction='none'
loss = nn.MSELoss(reduction='none')
output = loss(preds, targets)
print(output)
# 输出逐元素的平方误差：tensor([0.2500, 0.2500, 0.0000, 1.0000])
```

在这个例子中，`output` 返回了每个元素的平方误差。如果想对这些误差做进一步的处理（例如加权平均或条件筛选），可以基于 `output` 张量进行操作。